{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e7cb1f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lxml in /Users/stevenyuan/opt/anaconda3/lib/python3.8/site-packages (4.6.3)\r\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lxml'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/cw/463xpk7n6hndd1xjb9kt_whm0000gn/T/ipykernel_31973/3538696461.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mrandom\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 9\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mlxml\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mhtml\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mfromstring\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     10\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     11\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'lxml'"
     ]
    }
   ],
   "source": [
    "!pip3 install lxml\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "from lxml.html import fromstring\n",
    "\n",
    "\n",
    "randos = [\"https://sandiego.nextrequest.com/documents\",\"https://sandiego.nextrequest.com/requests/new\",\"https://sandiego.nextrequest.com/users/sign_in\"]\n",
    "headers = requests.utils.default_headers()\n",
    "headers.update({\n",
    "    'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:52.0) Gecko/20100101 Firefox/52.0',\n",
    "})\n",
    "\n",
    "def get_proxies():\n",
    "    url = 'https://free-proxy-list.net/'\n",
    "    response = requests.get(url)\n",
    "    parser = fromstring(response.text)\n",
    "    proxies = set()\n",
    "    for i in parser.xpath('//tbody/tr')[:10]:\n",
    "        if i.xpath('.//td[7][contains(text(),\"yes\")]'):\n",
    "            proxy = \":\".join([i.xpath('.//td[1]/text()')[0], i.xpath('.//td[2]/text()')[0]])\n",
    "            proxies.add(proxy)\n",
    "    return proxies\n",
    "\n",
    "\n",
    "PROXIES = list(get_proxies())\n",
    "\n",
    "def get_data(url):\n",
    "    \n",
    "    def cleanhtml(raw_html):\n",
    "        cleanr = re.compile('<.*?>')\n",
    "        cleantext = re.sub(cleanr, '', raw_html)\n",
    "        return cleantext\n",
    "\n",
    "    ids = url[-7:]\n",
    "    \n",
    "    rand_proxy = random.randrange(len(PROXIES))\n",
    "    proxy = {'http':'http://'+PROXIES[rand_proxy],'https':'https://'+PROXIES[rand_proxy]}\n",
    "    page = requests.get(url,headers = headers,proxies = proxy)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    dept = soup.find_all(class_=\"current-department\")\n",
    "    depts = cleanhtml(str(dept[0])).strip()\n",
    "\n",
    "    times = soup.find_all(class_=\"time-quotes\")\n",
    "\n",
    "    creation = cleanhtml(str(times[0])).strip()\n",
    "    closing = cleanhtml(str(times[-1])).strip()\n",
    "    \n",
    "    \n",
    "    def get_am_or_pm(time):\n",
    "        if(time.partition(\"pm\")[0][-2:] != 'am'):\n",
    "#             print(time.partition(\"pm\")[0][:-2],time.partition(\"pm\")[0][:-2]!=\"am\")\n",
    "            return time.partition(\"pm\")[0]+\"pm\"\n",
    "        return time.partition(\"pm\")[0]\n",
    "    \n",
    "    \n",
    "    \n",
    "    try:\n",
    "        doj_creation = datetime.datetime.strptime(get_am_or_pm(creation), '%B %d, %Y, %I:%M%p')\n",
    "        doj_closing  = datetime.datetime.strptime(get_am_or_pm(closing), '%B %d, %Y, %I:%M%p')\n",
    "    except(ValueError):\n",
    "        print(url)\n",
    "        print(get_am_or_pm(creation))\n",
    "        print(get_am_or_pm(closing))\n",
    "    time_to_close = (doj_creation - doj_closing)\n",
    "\n",
    "    return ids, depts, time_to_close\n",
    "\n",
    "\n",
    "ids = []\n",
    "depts = []\n",
    "time_to_close = []\n",
    "\n",
    "for i in range(21,22):\n",
    "    for j in range(500,540):\n",
    "#         time.sleep(15)\n",
    "        url = \"https://sandiego.nextrequest.com/requests/\"+str(i) + \"-\" + str(j)\n",
    "        rand_proxy = random.randrange(len(PROXIES))\n",
    "        proxy = {'http':'http://'+ PROXIES[rand_proxy],'https':'https://'+PROXIES[rand_proxy]}\n",
    "        if(j%19 ==0):\n",
    "            rand_index = random.randrange(len(randos))\n",
    "            requests.get(randos[rand_index])\n",
    "        elif(requests.get(url,headers = headers,proxies = proxy).url != 'https://sandiego.nextrequest.com/requests'):\n",
    "            print(url)\n",
    "            req_id, dept, t = get_data(url)\n",
    "            ids.append(req_id)\n",
    "            depts.append(dept)\n",
    "            time_to_close.append(t)\n",
    "\n",
    "\"\"\"\n",
    "TODO:\n",
    "ids = []\n",
    "depts = []\n",
    "time_to_close = []\n",
    "for url in urls:\n",
    "    id, dept, t = get_data(url)\n",
    "    ids.append(id)\n",
    "    depts.append(dept)\n",
    "    time_to_close.append(t)\n",
    "result_df = pd.DataFrame(data = {'Request ID': ids,\n",
    "                    'Department': depts,\n",
    "                    'Time to Close': time_to_close}) \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "result_df = pd.DataFrame(data = {'Request ID': ids,\n",
    "                    'Department': depts,\n",
    "                    'Time to Close': time_to_close})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a244c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "from lxml.html import fromstring\n",
    "\n",
    "\n",
    "randos = [\"https://sandiego.nextrequest.com/documents\",\"https://sandiego.nextrequest.com/requests/new\",\"https://sandiego.nextrequest.com/users/sign_in\"]\n",
    "headers = requests.utils.default_headers()\n",
    "headers.update({\n",
    "    'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:52.0) Gecko/20100101 Firefox/52.0',\n",
    "})\n",
    "\n",
    "def get_proxies():\n",
    "    url = 'https://free-proxy-list.net/'\n",
    "    response = requests.get(url)\n",
    "    parser = fromstring(response.text)\n",
    "    proxies = set()\n",
    "    for i in parser.xpath('//tbody/tr')[:10]:\n",
    "        if i.xpath('.//td[7][contains(text(),\"yes\")]'):\n",
    "            proxy = \":\".join([i.xpath('.//td[1]/text()')[0], i.xpath('.//td[2]/text()')[0]])\n",
    "            proxies.add(proxy)\n",
    "    return proxies\n",
    "\n",
    "\n",
    "# PROXIES = list(get_proxies())\n",
    "\n",
    "def get_data(url):\n",
    "    \n",
    "    def cleanhtml(raw_html):\n",
    "        cleanr = re.compile('<.*?>')\n",
    "        cleantext = re.sub(cleanr, '', raw_html)\n",
    "        return cleantext\n",
    "\n",
    "    ids = url[-7:]\n",
    "    \n",
    "    # rand_proxy = random.randrange(len(PROXIES))\n",
    "    # proxy = {'http':'http://'+PROXIES[rand_proxy],'https':'https://'+PROXIES[rand_proxy]}\n",
    "    page = requests.get(url,headers = headers)#,proxies = proxy)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    dept = soup.find_all(class_=\"current-department\")\n",
    "    depts = cleanhtml(str(dept[0])).strip()\n",
    "\n",
    "    times = soup.find_all(class_=\"time-quotes\")\n",
    "\n",
    "    creation = cleanhtml(str(times[0])).strip()\n",
    "    closing = cleanhtml(str(times[-1])).strip()\n",
    "    \n",
    "    \n",
    "    def get_am_or_pm(time):\n",
    "        if(time.partition(\"pm\")[0][-2:] != 'am'):\n",
    "#             print(time.partition(\"pm\")[0][:-2],time.partition(\"pm\")[0][:-2]!=\"am\")\n",
    "            return time.partition(\"pm\")[0]+\"pm\"\n",
    "        return time.partition(\"pm\")[0]\n",
    "    \n",
    "    \n",
    "    \n",
    "    try:\n",
    "        doj_creation = datetime.datetime.strptime(get_am_or_pm(creation), '%B %d, %Y, %I:%M%p')\n",
    "        doj_closing  = datetime.datetime.strptime(get_am_or_pm(closing), '%B %d, %Y, %I:%M%p')\n",
    "    except(ValueError):\n",
    "        print(url)\n",
    "        print(get_am_or_pm(creation))\n",
    "        print(get_am_or_pm(closing))\n",
    "    time_to_close = (doj_creation - doj_closing)\n",
    "\n",
    "    return ids, depts, time_to_close\n",
    "\n",
    "\n",
    "ids = []\n",
    "depts = []\n",
    "time_to_close = []\n",
    "\n",
    "for i in range(21,22):\n",
    "    for j in range(500,535):\n",
    "#         time.sleep(15)\n",
    "        url = \"https://sandiego.nextrequest.com/requests/\"+str(i) + \"-\" + str(j)\n",
    "        # rand_proxy = random.randrange(len(PROXIES))\n",
    "        # proxy = {'http':'http://'+ PROXIES[rand_proxy],'https':'https://'+PROXIES[rand_proxy]}\n",
    "        if(j%19 ==0):\n",
    "            rand_index = random.randrange(len(randos))\n",
    "            requests.get(randos[rand_index])\n",
    "        elif(requests.get(url,headers = headers).url != 'https://sandiego.nextrequest.com/requests'):\n",
    "            print(url)\n",
    "            req_id, dept, t = get_data(url)\n",
    "            ids.append(req_id)\n",
    "            depts.append(dept)\n",
    "            time_to_close.append(t)\n",
    "\n",
    "\"\"\"\n",
    "TODO:\n",
    "ids = []\n",
    "depts = []\n",
    "time_to_close = []\n",
    "for url in urls:\n",
    "    id, dept, t = get_data(url)\n",
    "    ids.append(id)\n",
    "    depts.append(dept)\n",
    "    time_to_close.append(t)\n",
    "result_df = pd.DataFrame(data = {'Request ID': ids,\n",
    "                    'Department': depts,\n",
    "                    'Time to Close': time_to_close}) \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "result_df = pd.DataFrame(data = {'Request ID': ids,\n",
    "                    'Department': depts,\n",
    "                    'Time to Close': time_to_close})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04e2b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROXIES = list(get_proxies())\n",
    "rand_proxy = random.randrange(len(PROXIES))\n",
    "proxy = {'http':'http://'+PROXIES[rand_proxy],'https':'https://'+PROXIES[rand_proxy]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d47db46",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca76da83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "from lxml.html import fromstring\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_data(url):\n",
    "    \n",
    "    def cleanhtml(raw_html):\n",
    "        cleanr = re.compile('<.*?>')\n",
    "        cleantext = re.sub(cleanr, '', raw_html)\n",
    "        return cleantext\n",
    "\n",
    "    ids = url[-6:]\n",
    "    \n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    dept = soup.find_all(class_=\"current-department\")\n",
    "    depts = cleanhtml(str(dept[0])).strip()\n",
    "\n",
    "    times = soup.find_all(class_=\"time-quotes\")\n",
    "\n",
    "    creation = cleanhtml(str(times[0])).strip()\n",
    "    closing = cleanhtml(str(times[-1])).strip()\n",
    "    \n",
    "    def get_am_or_pm(time):\n",
    "        if(time.partition(\"pm\")[0][-2:] != 'am'):\n",
    "            return time.partition(\"pm\")[0]+\"pm\"\n",
    "        return time.partition(\"pm\")[0]\n",
    "    \n",
    "    \n",
    "#     try:\n",
    "    doj_creation = datetime.datetime.strptime(get_am_or_pm(creation), '%B %d, %Y, %I:%M%p')\n",
    "    doj_closing  = datetime.datetime.strptime(get_am_or_pm(closing), '%B %d, %Y, %I:%M%p')\n",
    "#     except(ValueError):\n",
    "#         print(url)\n",
    "#         print(get_am_or_pm(creation))\n",
    "#         print(get_am_or_pm(closing))\n",
    "    time_to_close = (doj_creation - doj_closing)\n",
    "\n",
    "    return ids, depts, time_to_close\n",
    "\n",
    "\n",
    "ids = []\n",
    "depts = []\n",
    "time_to_close = []\n",
    "\n",
    "endings = ['21-' + str(x) for x in np.arange(10000)]\n",
    "for i in tqdm(endings):\n",
    "    url = \"https://sandiego.nextrequest.com/requests/\" + i \n",
    "    try:\n",
    "        req_id, dept, t = get_data(url)\n",
    "        ids.append(req_id)\n",
    "        depts.append(dept)\n",
    "        time_to_close.append(t)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "result_df = pd.DataFrame(data = {'Request ID': ids,'Department': depts,'Time to Close': time_to_close})\n",
    "\n",
    "result_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817f1e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace41484",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "PyCharm (q1)",
   "language": "python",
   "name": "pycharm-3e22716c"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}