{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b235fe7",
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in /Users/stevenyuan/opt/anaconda3/lib/python3.8/site-packages (3.141.0)\r\n",
      "Requirement already satisfied: urllib3 in /Users/stevenyuan/opt/anaconda3/lib/python3.8/site-packages (from selenium) (1.26.4)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n",
    "from selenium.webdriver.firefox.options import Options # For Sai: Change 'firefox' \n",
    "                                                    # to your desired browser e.g. chrome\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import sys\n",
    "import traceback\n",
    "from timeit import default_timer as timer\n",
    "from tqdm import tqdm\n",
    "from time import sleep\n",
    "\n",
    "from io import StringIO\n",
    "import re\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "# Options for the driver\n",
    "options = Options()\n",
    "options.headless = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4339360a",
   "metadata": {},
   "source": [
    "### TO-DO\n",
    "- Figure out how to scrape documents when there are navigation bars in the documents section\n",
    "- Move the scraper methods to a separate Python file\n",
    "- Figure out if [Selenium Grid](https://www.selenium.dev/documentation/grid/) can potentially improve the performance of the scraper\n",
    "- Run on Google Colab/Microsoft Azure/local desktop/some other place?\n",
    "- Preliminary EDA:\n",
    "    - Get close time(s) and reason(s)\n",
    "    - Fill in empty department info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ffa5b4",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    ". # Keep this cell to prevent the rest of the notebook from automatically running"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2025bda3",
   "metadata": {},
   "source": [
    "The following script uses the `selenium` library to, in theory, scrape every request from [San Diego's NextRequest database](https://sandiego.nextrequest.com/requests). It does so by using the fact that every request has its own unique URL i.e. the request with ID 'yy-xxxx' will be found at '.../requests/yy-xxxx'. From each request webpage, the following information is extracted:\n",
    "\n",
    "- `id` (str): ID of the request, yy-xxxx\n",
    "- `status` (str): Whether the request is opened or closed. Always takes on a value of either 'CLOSED' or 'OPEN'\n",
    "- `desc` (str): Description of the request provided by the requester\n",
    "- `date` (str): Initial request date\n",
    "- `depts` (str): Current departments assigned to the request (may not be the ones the requester had initially)\n",
    "- `docs` (DataFrame in CSV format): All documents attached to the request, if there are any, otherwise None. The columns are:\n",
    "    - `title` (str): Title given to each document\n",
    "    - `link` (str): Link to each document\n",
    "- `poc` (str): Point of contact\n",
    "- `msgs` (DataFrame in CSV format): All messages attached to the request. The columns are:\n",
    "    - `title` (str): Title of each message\n",
    "    - `item` (str): Message body\n",
    "    - `time` (str): Date of each message\n",
    "\n",
    "After a request is scraped, the next request can be navigated to by clicking on an arrow, and the scraper continues to run until the arrow cannot be found, either because the scraper has reached the last request in the database or due to a timeout. To address these potential timeouts, we stop the driver every time it cannot access a request, then restart it after a short delay, starting from the request that it timed out on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf39626d",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def scrape_requests_sequential(requests, driver, num_requests=-1, cooldown=1, debug=0, progress=0):\n",
    "    '''\n",
    "    Scrapes all records on a NextRequest request database starting from the given ID and\n",
    "    moving forward chronologically until the number of requests scraped reaches a given\n",
    "    number. Each scraped requests is added to a given list. If num_requests is non-positive, \n",
    "    then scrape as many records as possible.\n",
    "    '''\n",
    "    start = timer() # Timer for progress checking purposes\n",
    "    counter = 0 # Keeps track of how many requests have been scraped\n",
    "    \n",
    "    # Start by scraping the initial record. TO-DO: Add try-except-finally blocks for KeyboardInterrupt errors\n",
    "    \n",
    "    # Only scrape a request if it was loaded properly; otherwise, stop the scraper\n",
    "    if not driver.find_elements_by_class_name('nextrequest'):\n",
    "        print('No requests scraped')\n",
    "        return counter\n",
    "\n",
    "    scrape_request_append(requests, driver, counter=counter, debug=debug) # Scrape request\n",
    "\n",
    "    counter += 1\n",
    "    \n",
    "    # For positive num_requests, return the list of requests if the counter reaches the desired number\n",
    "    if ((num_requests > 0) and (counter == num_requests)):\n",
    "        if progress:\n",
    "            print_progress_final(counter, start, end=timer(), last_request=requests[-1]['id'])\n",
    "        \n",
    "        return counter\n",
    "\n",
    "    # Show progress, if desired\n",
    "    if progress and (counter % progress == 0):\n",
    "        print_progress(counter, start, end=timer())\n",
    "    \n",
    "    # Continue to scrape until it is not possible to navigate to the next request, \n",
    "    # either due to the scraper reaching the end of the database or because of a\n",
    "    # timeout\n",
    "    while driver.find_elements_by_class_name('js-next-request'): \n",
    "        driver.find_element_by_class_name('js-next-request').click() # Click on the arrow to navigate to the next request\n",
    "        sleep(cooldown) # Cooldown between scraping attempts\n",
    "        \n",
    "        if not driver.find_elements_by_class_name('nextrequest'):\n",
    "            break\n",
    "        \n",
    "        scrape_request_append(requests, driver, counter=counter, debug=debug) \n",
    "        \n",
    "        counter += 1\n",
    "        \n",
    "        if ((num_requests > 0) and (counter == num_requests)):\n",
    "            break\n",
    "        \n",
    "        if progress and (counter % progress == 0):\n",
    "            print_progress(counter, start, end=timer())\n",
    "    \n",
    "    # Final progress check\n",
    "    if progress:\n",
    "        print_progress_final(counter, start, end=timer(), last_request=requests[-1]['id'])\n",
    "        \n",
    "    return counter\n",
    "\n",
    "def scrape_request_append(requests, driver, counter=-1, debug=0):\n",
    "    '''\n",
    "    Scrapes data about a given request on a NextRequest request database, appending the result\n",
    "    to the given list.\n",
    "    '''\n",
    "    request_id, status, desc, date, depts, docs, poc, events = [None] * 8 # Initialize variables \n",
    "    try: # Attempt to scrape relevant data\n",
    "        request_id = driver.find_element_by_class_name('request-title-text').text.split()[1][1:] # Request ID\n",
    "        status = driver.find_element_by_class_name('request-status-label').text.strip() # Request status\n",
    "        \n",
    "        desc_row = driver.find_element_by_class_name('request-text') # Box containing request description\n",
    "        for desc_read_more in desc_row.find_elements_by_partial_link_text('Read more'): # Expand description if necessary\n",
    "            desc_read_more.click()\n",
    "        sleep(0.01) # TO-DO: Replace with a WebDriverWait\n",
    "        desc = desc_row.find_element_by_id('request-text').text # Full request description\n",
    "        \n",
    "        date = driver.find_element_by_class_name('request_date').text # Request date\n",
    "        depts = driver.find_element_by_class_name('current-department').text # Department(s) assigned to the request\n",
    "        poc = driver.find_element_by_class_name('request-detail').text # Person of contact\n",
    "\n",
    "        # Messages recorded on the request page, if there are any\n",
    "        event_history = driver.find_elements_by_class_name('generic-event') # All message blocks\n",
    "        if event_history: # Check for presence of messages\n",
    "            num_events = len(event_history)\n",
    "\n",
    "            # Titles, descriptions, and time strings for each message\n",
    "            event_titles = [None] * num_events\n",
    "            event_items = [None] * num_events\n",
    "            time_quotes = [None] * num_events\n",
    "\n",
    "            # Scrape information from each individual event\n",
    "            for i in range(len(event_history)):\n",
    "                event = event_history[i]\n",
    "\n",
    "                event_title = event.find_element_by_class_name('event-title').text # Event title\n",
    "                for details_toggle in event.find_elements_by_partial_link_text('Details'): # Expand event item details\n",
    "                    details_toggle.click()\n",
    "                event_item = '\\n'.join(get_webelement_text(event.find_elements_by_class_name('event-item'))) # Event item\n",
    "                time_quote = event.find_element_by_class_name('time-quotes').text # Time quote\n",
    "\n",
    "                event_titles[i] = event_title\n",
    "                event_items[i] = event_item\n",
    "                time_quotes[i] = time_quote\n",
    "\n",
    "            # DataFrame-converted-to-CSV consisting of all messages\n",
    "            events = pd.DataFrame({ \n",
    "                'title': event_titles,\n",
    "                'item': event_items,\n",
    "                'time': time_quotes\n",
    "                }).to_csv(index=False)\n",
    "            \n",
    "        # Documents attached to the request, if there are any (CURRENTLY DOES NOT SCRAPE ALL DOCUMENTS)\n",
    "        doc_list = driver.find_element_by_class_name('document-list') # Box containing documents\n",
    "        if '(none)' not in doc_list.text: # Check for the presence of documents\n",
    "            # Expand folders, if there are any\n",
    "            folders = doc_list.find_elements_by_class_name('folder-toggle') \n",
    "            for folder in folders:\n",
    "                folder.click()\n",
    "            sleep(0.01) # TO-DO: Replace with a WebDriverWait\n",
    "            \n",
    "            docs_all = doc_list.find_elements_by_class_name('document-link')\n",
    "            \n",
    "            # TO-DO: Figure out how to scrape all documents from a request whose folders also have navigation bars\n",
    "#             # If there are many documents, then there will be navigation bar(s)\n",
    "#             pag_navs = doc_list.find_elements_by_class_name('pagy-nav')\n",
    "#             if pag_navs:\n",
    "#                 pag_nav = pag_navs[-1]\n",
    "#                 while not pag_nav.find_elements_by_class_name('page.next.disable'):\n",
    "#                     pag_nav.find_element_by_partial_link_text('Next').click()\n",
    "#                     doc_list = driver.find_element_by_class_name('document-list')\n",
    "\n",
    "#                     doc_titles.extend(get_webelement_text(doc_list.find_elements_by_class_name('document-link')))\n",
    "#                     doc_links.extend(remove_download_from_urls(get_webelement_link(doc_list.find_elements_by_class_name('document-link'))))  \n",
    "\n",
    "#             doc_titles = list(set(doc_titles))\n",
    "#             doc_links = list(set(doc_links))\n",
    "            \n",
    "            # DataFrame-converted-to-CSV consisting of all documents\n",
    "            docs = pd.DataFrame({\n",
    "                'title': get_webelement_text(docs_all),\n",
    "                'link': remove_download_from_urls(get_webelement_link(docs_all))\n",
    "                }).to_csv(index=False)\n",
    "            \n",
    "        # For testing purposes, print a message whenever a request is successfully scraped\n",
    "        if debug:\n",
    "            print(request_id, 'scraped')\n",
    "    except: # If an exception occurs, print the stack trace\n",
    "        print('Exception occured' + (' at count ' + str(counter + 1) if counter >= 0 else '') + ':')\n",
    "        traceback.print_exc()\n",
    "        print()\n",
    "    finally: # Append the request to the list\n",
    "        requests.append({\n",
    "            'id': request_id,\n",
    "            'status': status,\n",
    "            'desc': desc,\n",
    "            'date': date,\n",
    "            'depts': depts,\n",
    "            'docs': docs,\n",
    "            'poc': poc,\n",
    "            'msgs': events\n",
    "            })\n",
    "\n",
    "# Scraper utilities\n",
    "def print_progress(counter, start, end):\n",
    "    '''\n",
    "    Prints scraper progress\n",
    "    '''\n",
    "    print('Requests scraped:', counter, \n",
    "          '\\tAvg runtime:', str(round((end - start)/counter, 2)) + 's/request', \n",
    "          '\\tTotal runtime:', str(round(end - start, 1)) + 's')\n",
    "    \n",
    "def print_progress_final(counter, start, end, last_request):\n",
    "    '''\n",
    "    Prints final scraper progress\n",
    "    '''\n",
    "    print('Total requests scraped:', counter, \n",
    "              '\\tAvg runtime:', str(round((end - start)/counter, 2)) + 's/request', \n",
    "              '\\tTotal runtime:', str(round(end - start, 1)) + 's')\n",
    "    print()\n",
    "    print('Last request scraped:', last_request)\n",
    "    print()\n",
    "\n",
    "def get_city_from_url(url):\n",
    "    '''\n",
    "    Finds the city name from the NextRequest URL.\n",
    "    '''\n",
    "    return re.match(r'(?<=https://)[a-zA-Z]*', url)[0]\n",
    "\n",
    "def get_webelement_text(webelement):\n",
    "    '''\n",
    "    Gets the text of each web element in a list, if such a list exists.\n",
    "    '''\n",
    "    return list(map(lambda x: x.text, webelement)) if webelement else []\n",
    "\n",
    "def get_webelement_link(webelement):\n",
    "    '''\n",
    "    Gets the link of each web element in a list, if such a list exists.\n",
    "    '''\n",
    "    return list(map(lambda x: x.get_attribute('href'), webelement)) if webelement else []\n",
    "\n",
    "def remove_download_from_urls(urls):\n",
    "    '''\n",
    "    Removes '/download' from the end of a list of URLs, if the list exists.\n",
    "    '''\n",
    "    return list(map(lambda url: re.match(r'.*(?=/download)', url)[0], urls)) if urls else []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3da70dc",
   "metadata": {},
   "source": [
    "Define important variables before running the scraper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af13da72",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "num_requests = -1 # Number of requests to scrape\n",
    "cooldown = 1 # Cooldown between request accesses. \n",
    "progress = 100 # Show progress every N requests that are scraped. \n",
    "timeout = 10 # Timeout wait time between scraper runs\n",
    "\n",
    "requests = [] # List of dictionaries containing scraped info on each request\n",
    "urls = ['https://lacity.nextrequest.com/requests/', \n",
    "        'https://nola.nextrequest.com/requests'] # URLs to scrape from. \n",
    "earliest_id = ['17-1', ] # Earliest IDs in the databases\n",
    "requests_name = ['lacity_requests', \n",
    "                 'nola_requests'] # Name of CSV file and ZIP archive to export scraped data to"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8738bb07",
   "metadata": {},
   "source": [
    "The following cell contains the main scraper process. Run it to start the scraper.\n",
    "\n",
    "**Important notes:**\n",
    "- If the scraper stops running for any reason, then simply rerun this cell to start it back up. \n",
    "- **Do not run the previous cell if the scraper stops, or else all your progress will be lost!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14597d66",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def scrape_nextrequest(requests, url, earliest_id, requests_name, num_requests=-1, cooldown=1, progress=100):\n",
    "    \"\"\"\n",
    "    Main scraper routine\n",
    "    \"\"\"\n",
    "    num_its = 1 # Keeps track of how many times the scraper has been run\n",
    "    current_id = requests[-1]['id'] if requests else earliest_id # Initialize the current ID to be either the earliest ID possible \n",
    "                                                                # if the requests list is empty, or the last ID in the list\n",
    "\n",
    "    # Instantiate headless (non-visible) Firefox driver\n",
    "    driver = webdriver.Firefox(options=options)\n",
    "\n",
    "    # Start by runnning an intial iteration of the scraper\n",
    "    driver.get(url + current_id)\n",
    "\n",
    "    # Print iteration number\n",
    "    it_num_title = 'Iteration ' + str(num_its)\n",
    "    print(it_num_title)\n",
    "    print('-' * len(it_num_title))\n",
    "\n",
    "    # Re-scrape the current request\n",
    "    start_id = requests.pop()['id'] if requests else current_id\n",
    "    print('Starting request:', start_id) \n",
    "    print()\n",
    "\n",
    "    # Scrape requests until the scraper either reaches the end of the database or times out\n",
    "    scrape_requests_sequential(requests, driver, \n",
    "                               num_requests=num_requests, \n",
    "                               cooldown=cooldown, \n",
    "                               progress=progress)\n",
    "\n",
    "    num_its += 1\n",
    "    sleep(timeout) # Wait after the script reaches the end of the database or after a timeout\n",
    "\n",
    "    # Restart the driver at the last request scraped\n",
    "    current_id = requests[-1]['id']\n",
    "    driver.get(url + current_id)\n",
    "\n",
    "    # Continue to scrape until the scraper reaches the end of the database or times out\n",
    "    while driver.find_elements_by_class_name('js-next-request'):\n",
    "        it_num_title = 'Iteration ' + str(num_its)\n",
    "        print(it_num_title)\n",
    "        print('-' * len(it_num_title))\n",
    "\n",
    "        print('Starting request:', requests.pop()['id'])\n",
    "        print()\n",
    "\n",
    "        scrape_requests_sequential(requests, driver, \n",
    "                                   num_requests=num_requests, \n",
    "                                   cooldown=cooldown, \n",
    "                                   progress=progress)\n",
    "\n",
    "        num_its += 1\n",
    "        sleep(timeout)\n",
    "\n",
    "        current_id = requests[-1]['id']\n",
    "        driver.get(url + current_id)\n",
    "\n",
    "    driver.close()\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    requests = [request for request in requests if (request and request['status'])]\n",
    "    requests_df = pd.DataFrame(requests).drop_duplicates()\n",
    "\n",
    "    # Create a zipped CSV file of the DataFrame\n",
    "    compression_opts = dict(method='zip', archive_name=requests_name + '.csv')\n",
    "    requests_df.to_csv('data/' + requests_name + '.zip', index=False, compression=compression_opts)\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63e297a",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# TO-DO: Convert this script into a function?\n",
    "\n",
    "# Initialize the current ID to be either the earliest ID possible if the requests list is empty, or the last ID in the list\n",
    "current_id = requests[-1]['id'] if requests else earliest_id \n",
    "\n",
    "# Instantiate headless (non-visible) Firefox driver. For Sai: Change 'Firefox' to your browser of choice e.g. Chrome\n",
    "driver = webdriver.Firefox(options=options)\n",
    "\n",
    "# Start by runnning an intial iteration of the scraper\n",
    "driver.get(url + current_id)\n",
    "\n",
    "# Print iteration number\n",
    "it_num_title = 'Iteration ' + str(num_its)\n",
    "print(it_num_title)\n",
    "print('-' * len(it_num_title))\n",
    "\n",
    "# Re-scrape the current request\n",
    "start_id = requests.pop()['id'] if requests else current_id\n",
    "print('Starting request:', start_id) \n",
    "print()\n",
    "\n",
    "# Scrape requests until the scraper either reaches the end of the database or times out\n",
    "scrape_requests_sequential(requests, driver, \n",
    "                           num_requests=num_requests, \n",
    "                           cooldown=cooldown, \n",
    "                           progress=progress)\n",
    "\n",
    "num_its += 1\n",
    "sleep(timeout) # Wait after the script reaches the end of the database or after a timeout\n",
    "\n",
    "# Restart the driver at the last request scraped\n",
    "current_id = requests[-1]['id']\n",
    "driver.get(url + current_id)\n",
    "\n",
    "# Continue to scrape until the scraper reaches the end of the database or times out\n",
    "while driver.find_elements_by_class_name('js-next-request'):\n",
    "    it_num_title = 'Iteration ' + str(num_its)\n",
    "    print(it_num_title)\n",
    "    print('-' * len(it_num_title))\n",
    "    \n",
    "    print('Starting request:', requests.pop()['id'])\n",
    "    print()\n",
    "    \n",
    "    scrape_requests_sequential(requests, driver, \n",
    "                               num_requests=num_requests, \n",
    "                               cooldown=cooldown, \n",
    "                               progress=progress)\n",
    "    \n",
    "    num_its += 1\n",
    "    sleep(timeout)\n",
    "\n",
    "    current_id = requests[-1]['id']\n",
    "    driver.get(url + current_id)\n",
    "\n",
    "driver.close()\n",
    "\n",
    "# Convert to DataFrame\n",
    "requests = [request for request in requests if (request and request['status'])]\n",
    "requests_df = pd.DataFrame(requests).drop_duplicates()\n",
    "\n",
    "# Create a zipped CSV file of the DataFrame\n",
    "compression_opts = dict(method='zip', archive_name=requests_name + '.csv')\n",
    "requests_df.to_csv('data/' + requests_name + '.zip', index=False, compression=compression_opts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163bf236",
   "metadata": {},
   "source": [
    "For Sai: **Run every code cell above this line (excluding the one with just the single period and comment)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95902765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "driver = webdriver.Firefox()\n",
    "print()\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ee2b6f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type('test') == str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74707787",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Convert to DataFrame\n",
    "requests = [request for request in requests if (request and request['status'])]\n",
    "requests_df = pd.DataFrame(requests).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b18c2a0",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Create a zipped CSV file of the DataFrame\n",
    "compression_opts = dict(method='zip', archive_name=requests_name + '.csv')\n",
    "requests_df.to_csv('data/' + requests_name + '.zip', index=False, compression=compression_opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06437da7",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Check to make sure the CSV file was properly created\n",
    "test_df = pd.read_csv(zipfile.ZipFile('data/sd_requests.zip', 'r').open('sd_requests.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba6a696",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c8a6a2",
   "metadata": {},
   "source": [
    "Scraper tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330e9d07",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "url = 'https://sandiego.nextrequest.com/requests/'\n",
    "\n",
    "test_requests = []\n",
    "driver = webdriver.Firefox()\n",
    "driver.get(url + '15-1810')\n",
    "\n",
    "scrape_requests_sequential(test_requests, driver, \n",
    "                           num_requests=11,\n",
    "                           debug=1,\n",
    "                           progress=2)\n",
    "\n",
    "driver.close()\n",
    "\n",
    "test = pd.DataFrame(test_requests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370ade7c",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f684b40",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "pd.read_csv(StringIO(test.loc[0]['msgs'])).tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2d6431",
   "metadata": {},
   "source": [
    "The following process converts the CSV strings in the `docs` and `msgs` columns into DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085e7801",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df_fillna = lambda df: df.convert_dtypes().fillna('') if df is not None else None\n",
    "test_df = df_fillna(test_df)\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45a23a1",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "csv_to_df = lambda csv: pd.read_csv(StringIO(csv)) if csv else None\n",
    "test_df['docs_df'] = test_df['docs'].apply(csv_to_df)\n",
    "test_df['msgs_df'] = test_df['msgs'].apply(csv_to_df)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72abc861",
   "metadata": {},
   "source": [
    "Then, we fill the NA values in the individual `docs` and `msgs` DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54545cbb",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "test_df['docs_df'] = test_df['docs_df'].apply(df_fillna)\n",
    "test_df['msgs_df'] = test_df['msgs_df'].apply(df_fillna)\n",
    "test_df.loc[4]['msgs_df']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11b2edc",
   "metadata": {},
   "source": [
    "Other EDA stuff:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23198310",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452a452f",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "test_df.shape[0] # Number of requests scraped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215499ef",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "test_df[test_df['desc'].str.contains('Read more')] # Check if the descriptions were properly scraped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc85ea8",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "empty_desc = test_df.query('desc == \"\"')\n",
    "empty_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ee706e",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Check for empty depts field\n",
    "empty_depts = test_df.query('depts == \"\"')\n",
    "empty_depts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1faa3e38",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Check for empty docs field\n",
    "empty_docs = test_df[test_df['docs'].str.fullmatch('title,link\\n')]\n",
    "empty_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f8e06a",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Which requests had the longest message history? (Useful for finding worst-case scenarios for the scraper)\n",
    "long_msgs = test_df['msgs_df'].apply(lambda df: df.shape[0] if df is not None else 0).sort_values(ascending=False)\n",
    "long_msgs.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078462ff",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Sort requests by message history length\n",
    "requests_long_msg = test_df.loc[long_msgs.index]\n",
    "requests_long_msg.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c817edd8",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Query for info about a specific request\n",
    "request_id = '\"17-3638\"'\n",
    "test_df.query('id == ' + request_id).iloc[0]['msgs_df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761620f1",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Find request descriptions with the given substring, case insensitive\n",
    "desc = 'Padres'\n",
    "test_df[test_df['desc'].str.contains(desc, case=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373c3c23",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Find requests whose department(s) contain the given substring, case insensitive\n",
    "dept = 'Police'\n",
    "test_df[test_df['depts'].str.contains(dept, case=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452c6717",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Convert empty dataframes from docs_df into None\n",
    "remove_empty = lambda df: None if ((df is None) or (type(df) == str and not df) or df.empty) else df\n",
    "test_df['docs_df'] = test_df['docs_df'].apply(remove_empty)\n",
    "test_df[test_df['docs'].str.fullmatch('title,link\\n')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e431e9",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Split the date and request method from the date column\n",
    "dates = test_df['date'].to_numpy()\n",
    "test_df = test_df.join(\n",
    "        pd.DataFrame(list(map(lambda x: x.split(' via '), dates)))\n",
    "    ).drop(\n",
    "        columns='date'\n",
    "    ).rename(\n",
    "        columns={0: 'date', 1: 'via'}\n",
    "    ).convert_dtypes()\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0984e4",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Split the time and author from the time quote on each message\n",
    "def split_time_author(msgs):\n",
    "    if msgs is None:\n",
    "        return None\n",
    "    time_quotes = msgs['time'].to_numpy()\n",
    "    time_author = pd.DataFrame(list(map(lambda x: x.split(' by '), time_quotes)))\n",
    "    return df_fillna(msgs.join(\n",
    "            time_author\n",
    "        ).drop(\n",
    "            columns='time'\n",
    "        ).rename(\n",
    "            columns={0: 'time', 1: 'by'}\n",
    "        ))\n",
    "\n",
    "test_df['msgs_df'] = test_df['msgs_df'].apply(split_time_author)\n",
    "test_df.loc[4]['msgs_df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9237034b",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Convert columns with time strings into DateTime\n",
    "def convert_time_to_dt(df, col='time'):\n",
    "    return df.assign(**{col + '_dt': pd.to_datetime(df[col])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c9b3d7",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Splitting departments for easier pivoting\n",
    "depts = test_df['depts'].to_numpy() # depts column\n",
    "test_df_depts = test_df.join(pd.DataFrame(list(map(lambda x: x.split(', '), depts)))) # Split departments into separate columns\n",
    "test_df_depts = test_df_depts.melt( # Melt on the individual departments\n",
    "        id_vars=test_df.columns\n",
    "    )[lambda df: df['value'].apply(lambda x: x is not None)].drop( # Get rid of None values\n",
    "        columns='variable'\n",
    "    ).rename( # Drop the variable column, rename the value column, and reset indices\n",
    "        columns={'value': 'dept'}\n",
    "    ).reset_index().drop(\n",
    "        columns='index'\n",
    "    )\n",
    "test_df_depts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadc38a4",
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_df_depts.value_counts('dept')[lambda x: x.index.str.contains('Office', case=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d36e1b0",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Find requests whose department(s) contain the given substring, case insensitive\n",
    "dept = 'Chief Operating Officer'\n",
    "test_df[test_df['depts'].str.contains(dept, case=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e021fc",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "test_df = test_df.convert_dtypes()\n",
    "test_df.loc[0]['msgs_df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5315e747",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "test.loc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68316b8f",
   "metadata": {},
   "source": [
    "Previous scraper function attempts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e483b03",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def scrape_record(url, request_id, driver):\n",
    "    '''\n",
    "    Scrapes data about a given request on a NextRequest request database\n",
    "    '''\n",
    "    driver.get(url + request_id) # Attempt to access the record\n",
    "#     timeout = 2 # Timeout length, in seconds\n",
    "    \n",
    "#     try:\n",
    "#         WebDriverWait(driver, timeout).until(EC.visibility_of_all_elements_located((By.CLASS_NAME, 'nextrequest')))\n",
    "#     except TimeoutException:\n",
    "#         print(request_id, 'timed out')\n",
    "#         pass\n",
    "    \n",
    "    if (request_id not in driver.title):\n",
    "        return\n",
    "    \n",
    "    status, desc, date, depts, docs, poc, events = [None] * 7 # Initialize variables \n",
    "    try: # Attempt to scrape relevant data\n",
    "        status = driver.find_element_by_class_name('request-status-label').text.strip() # Request status\n",
    "        desc = driver.find_element_by_class_name('request-text.row').text # Request description\n",
    "        date = driver.find_element_by_class_name('request_date').text # Request date\n",
    "        depts = driver.find_element_by_class_name('current-department').text # Department(s) assigned to the request\n",
    "        poc = driver.find_element_by_class_name('request-detail').text # Person of contact\n",
    "        \n",
    "        # Documents attached to the request, if there are any\n",
    "        public_docs = driver.find_element_by_id('document-list') # WebElement containing the documents\n",
    "        if '(none)' not in public_docs.text:\n",
    "            # Expand folders, if there are any\n",
    "            folders = public_docs.find_elements_by_class_name('folder-toggle') \n",
    "            if folders:\n",
    "                for folder in folders:\n",
    "                    folder.click()\n",
    "\n",
    "            doc_links = public_docs.find_elements_by_class_name('document-link') # Links to documents\n",
    "            \n",
    "            # DataFrame consisting of all documents\n",
    "            docs = pd.DataFrame({\n",
    "                'title': get_webelement_text(doc_links),\n",
    "                'link': remove_download_from_urls(get_webelement_link(doc_links))\n",
    "                })\n",
    "            \n",
    "        # Show all message history, if the option is available\n",
    "        show_all_history = driver.find_elements_by_class_name('show-all-history')\n",
    "        if show_all_history:\n",
    "            show_all_history[0].click()\n",
    "        \n",
    "        # Messages recorded on the request page, if there are any\n",
    "        event_history = driver.find_elements_by_class_name('generic-event')\n",
    "        if event_history:\n",
    "            num_events = len(event_history)\n",
    "            \n",
    "            # Titles, descriptions, and time strings for each message\n",
    "            event_titles = [None] * num_events\n",
    "            event_items = [None] * num_events\n",
    "            time_quotes = [None] * num_events\n",
    "            \n",
    "            # Scrape information from each individual event\n",
    "            for i in range(len(event_history)):\n",
    "                event = event_history[i]\n",
    "                \n",
    "                event_title = event.find_element_by_class_name('event-title').text\n",
    "                event_item = '\\n'.join(get_webelement_text(event.find_elements_by_class_name('event-item')))\n",
    "                time_quote = event.find_element_by_class_name('time-quotes').text\n",
    "                \n",
    "                event_titles[i] = event_title\n",
    "                event_items[i] = event_item\n",
    "                time_quotes[i] = time_quote\n",
    "                \n",
    "            # DataFrame consisting of all messages\n",
    "            events = pd.DataFrame({ \n",
    "                'title': event_titles,\n",
    "                'item': event_items,\n",
    "                'time': time_quotes\n",
    "                })\n",
    "    except NoSuchElementException: # Catch exception thrown if a specific element cannot be found, and silently pass\n",
    "        pass\n",
    "    except: # If some other exception occurs, print information about the exception\n",
    "        traceback.print_exc()\n",
    "    finally: # Return the request\n",
    "        return {\n",
    "            'id': request_id,\n",
    "            'status': status,\n",
    "            'desc': desc,\n",
    "            'date': date,\n",
    "            'depts': depts,\n",
    "            'docs': docs,\n",
    "            'poc': poc,\n",
    "            'msgs': events\n",
    "            }\n",
    "    \n",
    "def scrape_record_parallel(url, request_id):\n",
    "    '''\n",
    "    Scraper method used for parallelization\n",
    "    '''\n",
    "    driver = webdriver.Firefox(options=options)\n",
    "    request_info = scrape_record(url, request_id, driver)\n",
    "    driver.close()\n",
    "    return request_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294374c9",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Options for scraping\n",
    "earliest_year = 16 # Earliest year to search requests for\n",
    "latest_year = 21 # Latest year to search request for\n",
    "id_start = 1 # ID value to start from\n",
    "id_range = id_max # Number of IDs to try for each year\n",
    "cooldown = 0.9 # Amount of time, in seconds, to wait between website accesses\n",
    "\n",
    "start_id = '15-1810' # The request to start scraping from\n",
    "num_requests = -1 # Number of requests to scrape\n",
    "progress = 100 # Display a message every 100 requests successfully scraped\n",
    "\n",
    "# List of all request IDs\n",
    "request_ids = [str(year) + '-' + str(num) for num in range(id_start, id_range + id_start) \n",
    "                                       for year in range(earliest_year, latest_year + 1)]\n",
    "\n",
    "# URLs to extract data from\n",
    "urls = ['https://sandiego.nextrequest.com/requests/', 'https://oaklandca.nextrequest.com/requests/']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253e2722",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Iterative script\n",
    "driver = webdriver.Firefox(options=options\n",
    "                          ) # Headless (non-visible) WebDriver\n",
    "\n",
    "sd_requests = [] # List of dictionaries containing information on each request\n",
    "i = 0 # Index for URLs - may be useful later for scraping multiple sites\n",
    "\n",
    "for year in range(earliest_year, latest_year + 1):\n",
    "    for num in tqdm(range(id_start, id_range + id_start)):\n",
    "        # NextRequest request IDs are a two-digit year and a number, with a dash in between\n",
    "        request_id = str(year) + '-' + str(num)\n",
    "        \n",
    "        # Scrape record\n",
    "        sd_requests.append(scrape_record(urls[i], request_id, driver)) \n",
    "\n",
    "        # Cooldown\n",
    "        sleep(cooldown)\n",
    "    \n",
    "    # sleep(cooldown) # Cooldown\n",
    "        \n",
    "driver.close()\n",
    "\n",
    "sd_requests = [x for x in sd_requests if x['status'] is not None] # Remove entries with incomplete information\n",
    "sd_requests_df = pd.DataFrame(sd_requests) # Convert to DataFrame\n",
    "\n",
    "# Create a zipped CSV file of the data\n",
    "compression_opts = dict(method='zip', archive_name='sd_requests.csv')\n",
    "sd_requests_df.to_csv('data/sd_requests_2.zip', index=False, compression=compression_opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f3b130",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Parallelized script\n",
    "scrape_request = lambda i: scrape_record_parallel(urls[0], i)\n",
    "sd_requests = Parallel(n_jobs=-1, prefer='threads', verbose=10)(delayed(scrape_request)(request_id) for request_id in request_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae5fc71",
   "metadata": {},
   "source": [
    "Miscellaneous tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca844da",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# # Test remote WebDriver (run Selenium Grid instance locally before running this cell)\n",
    "# driver = webdriver.Remote(desired_capabilities=DesiredCapabilities.FIREFOX, options=options)\n",
    "# driver.get('http://www.google.com')\n",
    "# driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc0a275",
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # Test to make sure the driver works\n",
    "# \n",
    "# driver = webdriver.Firefox(options=options) # Instantiate a headless Firefox WebDriver\n",
    "# for url in urls:\n",
    "#     driver.get(url)\n",
    "#     print(driver.title)\n",
    "#     sleep(5)\n",
    "    \n",
    "# driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8220c94",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "# # Test for retrieving message info from a specific request\n",
    "# driver = webdriver.Firefox(options=options)\n",
    "# driver.get('https://sandiego.nextrequest.com/requests/21-4915')\n",
    "# print(driver.title)\n",
    "\n",
    "# event_titles = driver.find_elements_by_class_name('event-title')\n",
    "# event_items = driver.find_elements_by_class_name('event-item')\n",
    "# times = driver.find_elements_by_class_name('time-quotes')\n",
    "# for title, item, time in list(zip(event_titles, event_items, times)):\n",
    "#     print(title.text)\n",
    "#     print(item.text)\n",
    "#     print(time.text)\n",
    "#     print()\n",
    "\n",
    "# driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96755c02",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# # Scraping documents test\n",
    "# driver = webdriver.Firefox(options=options)\n",
    "\n",
    "# for request_id in [5369, 5313, 5374]:\n",
    "#     url = 'https://sandiego.nextrequest.com/requests/21-' + str(request_id)\n",
    "#     driver.get(url)\n",
    "\n",
    "#     docs = driver.find_element_by_id('public-docs')\n",
    "#     folders = docs.find_elements_by_class_name('folder-toggle')\n",
    "#     if folders:\n",
    "#         for folder in folders:\n",
    "#             folder.click()\n",
    "#     else:\n",
    "#         print('No folders found for', request_id)\n",
    "    \n",
    "#     doc_links = docs.find_elements_by_class_name('document-link')\n",
    "#     display(\n",
    "#         pd.DataFrame(\n",
    "#             list(zip(get_webelement_text(doc_links), remove_download_from_urls(get_webelement_link(doc_links)))),\n",
    "#             columns=['title', 'link']\n",
    "#         )\n",
    "#     )\n",
    "#     display(\n",
    "#         pd.DataFrame({\n",
    "#             'title': get_webelement_text(doc_links),\n",
    "#             'link': remove_download_from_urls(get_webelement_link(doc_links))\n",
    "#         })\n",
    "#     )\n",
    "\n",
    "# driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31a16da",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
